{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be3b0bb0-31b3-4ac6-94bd-3d7611daf0a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import ebooklib\n",
    "from ebooklib import epub\n",
    "import re\n",
    "import os\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import words, stopwords, names\n",
    "\n",
    "ENGLISH_WORDS = set(words.words())\n",
    "\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda93d1a-f423-4df7-befd-01aa14860b1d",
   "metadata": {},
   "source": [
    "## preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17962380-75c4-4c9a-afec-f8b3467e4e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_strings_until_limit(strings, min_length, max_length, test_for_max = 0):\n",
    "    merged_string = \"\"\n",
    "    merged_strings = []\n",
    "    \n",
    "    for s in strings:\n",
    "        if len(merged_string) <= min_length:\n",
    "            merged_string += s\n",
    "        \n",
    "        elif len(merged_string) > max_length and test_for_max<5:\n",
    "                splitParagraph = merged_string.split('.')\n",
    "                splitParagraphRePoint = []\n",
    "                for sp in splitParagraph:\n",
    "                    splitParagraphRePoint.append(sp+'.')\n",
    "                \n",
    "                merged = merge_strings_until_limit(splitParagraphRePoint, min_length, max_length, test_for_max+1)\n",
    "                merged_strings.extend(merged)\n",
    "                merged_string = s\n",
    "        else:\n",
    "            merged_strings.append(merged_string)\n",
    "            merged_string = s\n",
    "    \n",
    "    if merged_string:\n",
    "        merged_strings.append(merged_string)\n",
    "    \n",
    "    return merged_strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "14bca839-b730-47ea-9436-7643adb715c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_english_word(word):\n",
    "    # Initialize the Enchant English dictionary\n",
    "    return (word.lower() in ENGLISH_WORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f068ad9-5c8c-42a7-b833-ecb16f562587",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_epub_paragraphs(epub_file, ID):\n",
    "    book = epub.read_epub(epub_file)\n",
    "    paragraphs = []\n",
    "    \n",
    "    for item in book.get_items_of_type(ebooklib.ITEM_DOCUMENT):\n",
    "        content = item.get_content().decode('utf-8')\n",
    "        content = re.sub('<[^<]+?>', '', content)\n",
    "        content = re.sub('\\s+', ' ', content)\n",
    "        content = re.sub('\\n', ' ', content)\n",
    "        \n",
    "        paragraphs.extend(content.strip().split(\"&#13;\"))\n",
    "    \n",
    "    paragraphs = merge_strings_until_limit(paragraphs, 200, 1000)\n",
    "    paragraphs = [{'paragraph':paragraphs[i], 'nr':i, 'bookID':ID} for i in range(len(paragraphs))]\n",
    "    \n",
    "    return paragraphs[1:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "eae041de-3bcc-4d13-84cf-ddfc45696d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(paragraphs):\n",
    "    processed_docs=[]\n",
    "    \n",
    "    for paragraph in paragraphs:\n",
    "        words = gensim.utils.simple_preprocess(paragraph, min_len = 3, deacc=True)\n",
    "        \n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
    "        \n",
    "        STOP_WORDS = stopwords.words(\"english\")\n",
    "        filtered_words = [word for word in lemmatized_words if ((word not in STOP_WORDS) and is_english_word(word))]\n",
    "        stemmer = PorterStemmer()\n",
    "        stemmed_words = [stemmer.stem(word) for word in filtered_words]\n",
    "        \n",
    "        processed_doc = \" \".join(stemmed_words)\n",
    "        \n",
    "        processed_docs.append({'processed_doc': processed_doc})\n",
    "    return processed_docs[1:-1] if len(processed_docs) > 1 else processed_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "99011215-7380-477d-a42a-7c1a5194e092",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\24171\\.conda\\envs\\RC11_Skill\\Lib\\site-packages\\ebooklib\\epub.py:1395: UserWarning: In the future version we will turn default option ignore_ncx to True.\n",
      "  warnings.warn('In the future version we will turn default option ignore_ncx to True.')\n",
      "C:\\Users\\24171\\.conda\\envs\\RC11_Skill\\Lib\\site-packages\\ebooklib\\epub.py:1423: FutureWarning: This search incorrectly ignores the root element, and will be fixed in a future version.  If you rely on the current behaviour, change it to './/xmlns:rootfile[@media-type]'\n",
      "  for root_file in tree.findall('//xmlns:rootfile[@media-type]', namespaces={'xmlns': NAMESPACES['CONTAINERNS']}):\n"
     ]
    }
   ],
   "source": [
    "paragraphs = read_epub_paragraphs('epubs/In the Beginning Was the Comman - Neal Stephenson.epub', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b2070fea-1684-4eb5-90c0-e9887f002de1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'processed_docs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m processed_docs \u001b[38;5;241m=\u001b[39m preprocess(p[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparagraph\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m paragraphs)\n",
      "Cell \u001b[1;32mIn[6], line 17\u001b[0m, in \u001b[0;36mpreprocess\u001b[1;34m(paragraphs)\u001b[0m\n\u001b[0;32m     13\u001b[0m     stemmed_words \u001b[38;5;241m=\u001b[39m [stemmer\u001b[38;5;241m.\u001b[39mstem(word) \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m filtered_words]\n\u001b[0;32m     15\u001b[0m     processed_doc \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(stemmed_words)\n\u001b[1;32m---> 17\u001b[0m     processed_docs\u001b[38;5;241m.\u001b[39mappend({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprocessed_doc\u001b[39m\u001b[38;5;124m'\u001b[39m: processed_doc})\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m processed_docs[\u001b[38;5;241m1\u001b[39m:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(processed_docs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m processed_docs\n",
      "\u001b[1;31mNameError\u001b[0m: name 'processed_docs' is not defined"
     ]
    }
   ],
   "source": [
    "processed_docs = preprocess(p['paragraph'] for p in paragraphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cdb3e5c-12f6-4b10-b404-f3fe2c106a40",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "processed_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdbeac2d-abee-422d-861b-0ebc2e438e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_docs=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "537ebf61-5ec6-492a-9e0b-61464202b9a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_paragraphs(query_text, n):\n",
    "    import gensim\n",
    "    from nltk.corpus import stopwords\n",
    "    from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "    from langdetect import detect  # 用于检查语言\n",
    "    from ebooklib import epub\n",
    "    from bs4 import BeautifulSoup\n",
    "    \n",
    "    # 全局列表，用于存储所有书籍的处理结果\n",
    "    all_processed_docs = []\n",
    "    \n",
    "    # 读取 epub 文件中的段落\n",
    "    def read_epub_paragraphs(epub_file, ID):\n",
    "        # 读取 epub 文件\n",
    "        book = epub.read_epub(epub_file)\n",
    "        paragraphs = []\n",
    "        \n",
    "        # 遍历章节，提取文本\n",
    "        for item in book.get_items():\n",
    "            if item.get_type() == ebooklib.ITEM_DOCUMENT:\n",
    "                soup = BeautifulSoup(item.content, 'html.parser')\n",
    "                for para in soup.find_all('p'):\n",
    "                    paragraphs.append(para.get_text())\n",
    "        \n",
    "        return paragraphs\n",
    "    \n",
    "    # 判断段落是否是英语文本\n",
    "    def is_english_text(text):\n",
    "        try:\n",
    "            # 使用 langdetect 来检测语言\n",
    "            return detect(text) == 'en'\n",
    "        except:\n",
    "            return False\n",
    "    \n",
    "    # 处理每本书的文本\n",
    "    def readingprocess(epub_file, ID):\n",
    "        paragraphs = read_epub_paragraphs(epub_file, ID)\n",
    "        \n",
    "        processed_docs = []  # 临时存储每本书的处理结果\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        stemmer = PorterStemmer()\n",
    "        STOP_WORDS = stopwords.words(\"english\")\n",
    "        \n",
    "        for i, paragraph in enumerate(paragraphs):\n",
    "            if is_english_text(paragraph):  # 确保段落是英文\n",
    "                # 分词处理\n",
    "                words = gensim.utils.simple_preprocess(paragraph, min_len=3, deacc=True)\n",
    "                \n",
    "                # 词形还原\n",
    "                lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
    "                \n",
    "                # 去除停用词\n",
    "                filtered_words = [word for word in lemmatized_words if word not in STOP_WORDS]\n",
    "                \n",
    "                # 词干提取（可选择是否需要）\n",
    "                stemmed_words = [stemmer.stem(word) for word in filtered_words]\n",
    "                \n",
    "                # 确保处理后的文档不为空\n",
    "                if len(stemmed_words) > 0:\n",
    "                    # 拼接为一个字符串\n",
    "                    processed_doc = \" \".join(stemmed_words)\n",
    "                    # 将处理后的文档加入临时列表\n",
    "                    #processed_docs.append({'processed_doc': processed_doc, 'bookID': ID})\n",
    "                    processed_docs.append({'processed_doc': processed_doc, 'nr': i, 'bookID': ID})\n",
    "                else:\n",
    "                    # 如果该段落处理后没有有效单词，跳过或打印警告\n",
    "                    print(f\"Warning: Paragraph from Book ID {ID} resulted in an empty processed document.\")\n",
    "        \n",
    "        # 返回处理后的文档\n",
    "        return processed_docs\n",
    "        \n",
    "\n",
    "\n",
    "    \n",
    "    # 调用处理函数并将结果添加到全局列表\n",
    "    def process_books():\n",
    "        # 依次处理每本书\n",
    "        all_processed_docs.extend(readingprocess('epubs/The Art-Architecture Complex - Hal Foster.epub', 1))\n",
    "        all_processed_docs.extend(readingprocess('epubs/About Looking - John Berger.epub', 2))\n",
    "        all_processed_docs.extend(readingprocess('epubs/Blowup - Michelle Kasprzak ed.epub', 3))\n",
    "    \n",
    "    # 执行处理\n",
    "    process_books()\n",
    "\n",
    "    \n",
    "    #矢量化\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer #形成tfidf矩阵\n",
    "    texts = [doc['processed_doc'] for doc in all_processed_docs]\n",
    "    vectorizer = TfidfVectorizer(min_df=2) #形成tfidf矩阵\n",
    "    tfidf_matrix = vectorizer.fit_transform(texts) #形成tfidf矩阵\n",
    "    \n",
    "    #导入相似度\n",
    "    from sklearn.metrics.pairwise import cosine_similarity #导入相似度\n",
    "    \n",
    "    #降维\n",
    "    import numpy as np\n",
    "    from scipy.sparse import random\n",
    "    from sklearn.decomposition import TruncatedSVD # also known as Latent Semantic Analysis (LSA)\n",
    "    n_components = 100 #独特词减少到100个，即保留最重要的100个独特词\n",
    "    svd = TruncatedSVD(n_components=n_components, algorithm = 'randomized') #这是计算 SVD 分解的更快、随机的方法\n",
    "    reduced_matrix = svd.fit_transform(tfidf_matrix) #得到一个形状为(n_samples, n_components)的矩阵，其中n_samples是段落数量，n_components（独特词）数量为 100\n",
    "    #前两行的作用是制定降维的规则\n",
    "    \n",
    "    #处理&调用查询词\n",
    "    def preprocess_query(query_text):\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        stemmer = PorterStemmer()\n",
    "        STOP_WORDS = stopwords.words(\"english\")\n",
    "\n",
    "        # 分词\n",
    "        words = gensim.utils.simple_preprocess(query_text, min_len=3, deacc=True)\n",
    "\n",
    "        # 词形还原\n",
    "        lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
    "\n",
    "        # 去除停用词\n",
    "        filtered_words = [word for word in lemmatized_words if word not in STOP_WORDS]\n",
    "\n",
    "        # 词干提取（可选择是否需要）\n",
    "        stemmed_words = [stemmer.stem(word) for word in filtered_words]\n",
    "\n",
    "        # 返回处理后的文本\n",
    "        return \" \".join(stemmed_words)\n",
    "        \n",
    "    processedQuery = preprocess_query(query_text) #把词语按照处理书籍的方式还原和筛选词汇\n",
    "    query_vector = vectorizer.transform([processedQuery]) #将处理过的词汇转为向量\n",
    "    query_vector.toarray().flatten().argsort()[::-1]\n",
    "    #对文档或查询中的术语的重要性进行排序。\n",
    "    #获取向量（例如 TF-IDF 向量）中最重要的术语的索引。\n",
    "    #确定查询中具有最高值的顶级特征（单词）\n",
    "    reduced_query_vec = svd.transform(query_vector) #将之前的向量转为降维的向量\n",
    "    similarities2 = cosine_similarity(reduced_query_vec, reduced_matrix) #查询降维后向量与每一个段落（降维后矩阵中的每一行）之间的相似度\\\n",
    "    \n",
    "    top_n = int(n)\n",
    "    top_n_indices = similarities2[0].argsort()[::-1][:top_n]  # 获取前四个最相似文档的索引\n",
    "    \n",
    "    # 打印前四个最相似文档的索引和相似度得分\n",
    "    print(f\"Top {top_n} most similar documents to the query:\")\n",
    "    for i in top_n_indices:\n",
    "        print(f\"Document Index: {i}\")\n",
    "        print(f\"Document: {all_processed_docs[i]['processed_doc']}\")\n",
    "        print(f\"Document: {all_processed_docs[i]['bookID']}\")\n",
    "        print(f\"Document: {all_processed_docs[i]['nr']}\")\n",
    "        print()\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "58d07022-9c8c-4b4a-a1e6-ccd17bce73a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\24171\\.conda\\envs\\RC11_Skill\\Lib\\site-packages\\ebooklib\\epub.py:1395: UserWarning: In the future version we will turn default option ignore_ncx to True.\n",
      "  warnings.warn('In the future version we will turn default option ignore_ncx to True.')\n",
      "C:\\Users\\24171\\.conda\\envs\\RC11_Skill\\Lib\\site-packages\\ebooklib\\epub.py:1423: FutureWarning: This search incorrectly ignores the root element, and will be fixed in a future version.  If you rely on the current behaviour, change it to './/xmlns:rootfile[@media-type]'\n",
      "  for root_file in tree.findall('//xmlns:rootfile[@media-type]', namespaces={'xmlns': NAMESPACES['CONTAINERNS']}):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Paragraph from Book ID 1 resulted in an empty processed document.\n",
      "Warning: Paragraph from Book ID 1 resulted in an empty processed document.\n",
      "Warning: Paragraph from Book ID 1 resulted in an empty processed document.\n",
      "Warning: Paragraph from Book ID 3 resulted in an empty processed document.\n",
      "Warning: Paragraph from Book ID 3 resulted in an empty processed document.\n",
      "Top 6 most similar documents to the query:\n",
      "Document Index: 1412\n",
      "Document: nephew like chequ unknown rel moneyvil\n",
      "Document: 2\n",
      "Document: 147\n",
      "\n",
      "Document Index: 2051\n",
      "Document: rememb wa like sung sleep fortun memori recent childhood repeat line word music like path path circular ring make link togeth like chain walk along path led circl lead one away field upon walk upon chain laid song\n",
      "Document: 2\n",
      "Document: 859\n",
      "\n",
      "Document Index: 1706\n",
      "Document: brecht wrote poem call crush impact citi end like\n",
      "Document: 2\n",
      "Document: 485\n",
      "\n",
      "Document Index: 1874\n",
      "Document: friend inform turner mother like snowstorm paint turner remark paint understood wish show scene wa like got sailor lash mast observ wa lash four hour expect escap felt bound record one busi like pictur\n",
      "Document: 2\n",
      "Document: 666\n",
      "\n",
      "Document Index: 2059\n",
      "Document: field hillsid seen either like tabl top inclin hill appear tilt field toward like music music stand effect perspect reduc minimum relat distant near equal one\n",
      "Document: 2\n",
      "Document: 867\n",
      "\n",
      "Document Index: 1703\n",
      "Document: famili fasanella centr mother right wall one paint father iceman crucifi wall brick head clamp ice tong work back wall second paint time mother sister stand chair front anoth wooden cross brick wall window frame everi person object kitchen memori happen within famili way paint truth experi primit paint method reveal way paint make everyth continu entir homogen exterior wall elev surround linoleum paint like street wall food dresser shelf like shop window display bare electr light bulb like street lamp electr meter like water hydrant back chair like rail\n",
      "Document: 2\n",
      "Document: 482\n",
      "\n"
     ]
    }
   ],
   "source": [
    "search_paragraphs('running like ghost', 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1330abc-ede4-435f-8487-f72813cbdfb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def readingprocess(epub_file, ID):\n",
    "    paragraphs = read_epub_paragraphs(epub_file, ID)\n",
    "    \n",
    "    processed_docs = []  # 存储每本书的处理结果\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stemmer = PorterStemmer()\n",
    "    STOP_WORDS = stopwords.words(\"english\")\n",
    "    \n",
    "    for i, paragraph in enumerate(paragraphs):\n",
    "        if is_english_text(paragraph):  # 确保段落是英文\n",
    "            # 分词\n",
    "            words = gensim.utils.simple_preprocess(paragraph, min_len=3, deacc=True)\n",
    "            \n",
    "            # 词形还原\n",
    "            lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
    "            \n",
    "            # 打印词形还原后的结果\n",
    "            print(f\"Original words: {words}\")\n",
    "            print(f\"Lemmatized words: {lemmatized_words}\")\n",
    "            \n",
    "            # 去除停用词\n",
    "            filtered_words = [word for word in lemmatized_words if word not in STOP_WORDS]\n",
    "            \n",
    "            # 打印去除停用词后的结果\n",
    "            print(f\"Filtered words: {filtered_words}\")\n",
    "            \n",
    "            # 词干提取（可选择是否需要）\n",
    "            stemmed_words = [stemmer.stem(word) for word in filtered_words]\n",
    "            \n",
    "            # 打印词干提取后的结果\n",
    "            print(f\"Stemmed words: {stemmed_words}\")\n",
    "            \n",
    "            # 如果该段落处理后有有效单词，加入结果\n",
    "            if len(stemmed_words) > 0:\n",
    "                processed_doc = \" \".join(stemmed_words)\n",
    "                processed_docs.append({'processed_doc': processed_doc, 'nr': i, 'bookID': ID})\n",
    "            else:\n",
    "                # 如果该段落处理后没有有效单词，跳过\n",
    "                print(f\"Warning: Paragraph from Book ID {ID} resulted in an empty processed document.\")\n",
    "    \n",
    "    return processed_docs\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
